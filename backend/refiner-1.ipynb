{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ.get(\"CO_API_KEY\")\n",
    "co = cohere.ClientV2(CO_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_RUBRIK = '''\n",
    "\n",
    "Given a prompt, can you please improve it so that it follows the Prompt Rubric:\n",
    "\n",
    "1. First, break the prompt into sequential steps: Eg. First [do something], then [do something]. Finally, [do something].\n",
    "\n",
    "2. Then, provide necessary context or background information. Eg. Report card. Which report card? For elementary students in Brazil?\n",
    "\n",
    "3. Afterwards, assign a role to the AI model. Eg. Imagine you are a teacher and I am a student. Please explain Newton’s Law.\n",
    "\n",
    "4. After this, specify the tone (Please use constructive tone, related to roles: whose perspective\n",
    "should the response be written from and who is the audience.). If possible, use action words to be more specific.\n",
    "\n",
    "5. If there are no examples, add an example when applicable.\n",
    "\n",
    "6. Explicitly say format of the response expected (bullet points, tables, code, etc).\n",
    "\n",
    "7. Include this at the end of the prompt: “Please do not assume anything, ask me any\n",
    "clarification questions if need be.\n",
    "\n",
    "8. If there is code included, label the prompt at the top and include ALL the code below the prompt.\n",
    "\n",
    "9. Finally, please do not apply it to this prompt. Memorize the Prompt Rubric for future prompts.\n",
    "'''\n",
    "PROMPT_ADDON = '''\n",
    "\\n Improve this prompt using the Prompt Rubric and Context Rubric, with the response being a string of the new prompt only. \n",
    "\n",
    "If there is code included, put the prompt at the top and include ALL the code below the prompt.\n",
    "\n",
    "Do not include **Improved Prompt**\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_context(prompt_type: str) -> str:\n",
    "    if prompt_type == 'EDUCATIONAL':\n",
    "        return '''You are to memorize the following Context Rubric for future prompts only.\n",
    "        Context Rubric:\n",
    "        1. If the prompt asks for a direct answer, implement guide rails in the prompt to prevent a direct answer.\n",
    "        2. Ensure the prompt will activate chain-of-thought prompting when obtaining a response.'''\n",
    "    elif prompt_type == 'PROBLEM-SOLVING':\n",
    "        return '''You are to memorize the following Context Rubric for future prompts only.\n",
    "        Context Rubric:\n",
    "        1. If the prompt asks for a direct answer or multiple choice, use zero-shot prompting in the prompt\n",
    "        2. If the prompt asks to solve a complicated problem, use meta prompting in the prompt'''\n",
    "    else: # prompt_type == EVERYDAY\n",
    "        return '''You are to memorize the following Context Rubric for future prompts only.\n",
    "        Context Rubric:\n",
    "        1. Ensure the prompt will activate few-shot prompting when obtaining a response\n",
    "        2. Make the prompt elicit a concise response without overcomplicating it.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TYPE = engineer_context('PROBLEM-SOLVING')\n",
    "user_prompt = '''prove CLT'''\n",
    "USER_PROMPT_FINAL = user_prompt + PROMPT_ADDON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin next\n",
      "\n",
      "Understood! I’ll memorize the **Prompt Rubric** for future prompts and ensure they are structured as follows:\n",
      "\n",
      "1. **Break into sequential steps**: Clearly outline the steps to be followed (e.g., First [do something], then [do something], finally [do something]).  \n",
      "2. **Provide necessary context**: Include relevant background information (e.g., specific details about the subject, audience, or scenario).  \n",
      "3. **Assign a role to the AI model**: Define the AI’s role (e.g., teacher, consultant, writer) and the user’s role (e.g., student, client, reader).  \n",
      "4. **Specify the tone**: Indicate the tone and perspective (e.g., constructive, formal, conversational) and the intended audience.  \n",
      "5. **Add examples if applicable**: Include examples to clarify expectations.  \n",
      "6. **Specify the response format**: Request the output format (e.g., bullet points, paragraphs, tables, code).  \n",
      "7. **Include clarification request**: End with “Please do not assume anything, ask me any clarification questions if need be.”  \n",
      "8. **Label and include code if applicable**: If code is involved, label the prompt and include all code below it.  \n",
      "\n",
      "I’ll apply this rubric to future prompts as needed. Let me know if you’d like me to refine or expand on any part of it!\n",
      "begin next\n",
      "\n",
      "Understood! I will apply the **Context Rubric** to future prompts as follows:\n",
      "\n",
      "1. **If the prompt asks for a direct answer or multiple choice**, I will use **zero-shot prompting** in the response.  \n",
      "2. **If the prompt asks to solve a complicated problem**, I will use **meta prompting** in the response.  \n",
      "\n",
      "I’ll ensure to adhere to this rubric for all subsequent interactions. Let me know if you have any further instructions or prompts!\n",
      "begin next\n",
      "\n",
      "\"Provide a rigorous mathematical proof of the Central Limit Theorem (CLT), including the necessary assumptions, key steps, and convergence in distribution. Explain the theorem's significance in probability theory and statistics, and illustrate its application with a practical example.\"\n"
     ]
    }
   ],
   "source": [
    "pipeline = [PROMPT_RUBRIK, PROMPT_TYPE, USER_PROMPT_FINAL]\n",
    "\n",
    "for step in pipeline:\n",
    "    response = co.chat(model=\"command-a-03-2025\", messages=[{\"role\": \"user\", \"content\": step}])\n",
    "    print('begin next\\n')\n",
    "    print(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = '''explain how to implement Djikstra’s algorithm in C'''\n",
    "prompt = '''explain object-oriented programming'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt + 'Improve this prompt using the Prompt Rubric, with the response being a string of the new prompt only.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Explain the core principles of object-oriented programming (OOP), including encapsulation, inheritance, polymorphism, and abstraction, and provide examples of how these concepts are applied in real-world programming scenarios.\"\n"
     ]
    }
   ],
   "source": [
    "response = co.chat(model=\"command-a-03-2025\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(response.message.content[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
